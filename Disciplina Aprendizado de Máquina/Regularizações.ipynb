{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizações / Coeficientes\n",
    "- l1\n",
    "- l2\n",
    "- elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados e Train e Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels [1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wine.data', header=None)\n",
    "\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                'Proline']\n",
    "\n",
    "print('Class labels', np.unique(df['Class label']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, 1:].values, df.iloc[:,0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (178, 13)\n",
      "np.unique(y):[1 2 3]\n",
      "accuracy_score:1.0\n",
      "intercept_: [-1.26393733 -1.21594878 -2.36997195]\n",
      "coef_: [[ 1.24655602  0.17985511  0.7465467  -1.16451486  0.          0.\n",
      "   1.15762945  0.          0.          0.          0.          0.56062\n",
      "   2.50835351]\n",
      " [-1.53750917 -0.3869622  -0.99531308  0.36477996 -0.05931227  0.\n",
      "   0.66812941  0.          0.         -1.93437381  1.23359092  0.\n",
      "  -2.2314704 ]\n",
      " [ 0.13531688  0.16916055  0.35755735  0.          0.          0.\n",
      "  -2.43499952  0.          0.          1.56327034 -0.81802688 -0.49540936\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "model_l1 = linear_model.LogisticRegression(penalty='l1', random_state=0)\n",
    "model_l1.fit(X_train_std, y_train)\n",
    "y_pred = model_l1.predict(X_test_std)\n",
    "print('X.shape: {}'.format(X.shape))\n",
    "print('np.unique(y):{}'.format(np.unique(y)))\n",
    "print('accuracy_score:{}\\nintercept_: {}\\ncoef_: {}'.format(accuracy_score(y_test, y_pred), model_l1.intercept_, model_l1.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (178, 13)\n",
      "np.unique(y):[1 2 3]\n",
      "accuracy_score:1.0\n",
      "intercept_: [-1.33509136 -0.96973542 -2.04898785]\n",
      "coef_: [[ 1.27713853  0.38210274  0.8015599  -1.30842842  0.22782837  0.23101419\n",
      "   0.90234371 -0.08423823  0.01462196 -0.0312838   0.02796323  0.71703048\n",
      "   1.79262118]\n",
      " [-1.45395571 -0.620303   -1.05445248  0.67148394 -0.29048951  0.18277571\n",
      "   0.51163918  0.10789643  0.08199321 -1.61228834  0.88800662  0.1659356\n",
      "  -1.73246957]\n",
      " [ 0.38965148  0.4083047   0.40211468  0.26242969  0.15288658 -0.20064653\n",
      "  -1.38792256 -0.06305419 -0.28440345  1.2553389  -0.93849662 -0.83821807\n",
      "   0.13754706]]\n"
     ]
    }
   ],
   "source": [
    "model_l2 = linear_model.LogisticRegression(penalty='l2', random_state=0)\n",
    "model_l2.fit(X_train_std, y_train)\n",
    "y_pred = model_l2.predict(X_test_std)\n",
    "print('X.shape: {}'.format(X.shape))\n",
    "print('np.unique(y):{}'.format(np.unique(y)))\n",
    "print('accuracy_score:{}\\nintercept_: {}\\ncoef_: {}'.format(accuracy_score(y_test, y_pred), model_l2.intercept_, model_l2.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (178, 13)\n",
      "np.unique(y):[1 2 3]\n",
      "accuracy_score:1.0\n",
      "intercept_: [ 0.36862533  0.78036204 -1.14898737]\n",
      "coef_: [[ 0.78792639  0.24089338  0.4473384  -0.73489829  0.1113343   0.22607273\n",
      "   0.60652205 -0.14847572  0.24131476  0.14140959  0.10670518  0.58061102\n",
      "   0.98240089]\n",
      " [-0.95393153 -0.43668273 -0.76795841  0.50646632 -0.17622299  0.07110642\n",
      "   0.38809799  0.09343743  0.11353057 -0.91231356  0.61090399  0.16780023\n",
      "  -1.10002567]\n",
      " [ 0.16600514  0.19578935  0.32062001  0.22843197  0.06488869 -0.29717916\n",
      "  -0.99462004  0.05503829 -0.35484533  0.77090397 -0.71760916 -0.74841125\n",
      "   0.11762478]]\n"
     ]
    }
   ],
   "source": [
    "model_multinomial = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty='l2', random_state=0)\n",
    "model_multinomial.fit(X_train_std, y_train)\n",
    "y_pred = model_multinomial.predict(X_test_std)\n",
    "print('X.shape: {}'.format(X.shape))\n",
    "print('np.unique(y):{}'.format(np.unique(y)))\n",
    "print('accuracy_score:{}\\nintercept_: {}\\ncoef_: {}'.format(accuracy_score(y_test, y_pred), model_multinomial.intercept_, model_multinomial.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:1.0\n",
      "intercept_:\n",
      "[-142.68148912 -125.44735328 -276.67748864]\n",
      "coef_:\n",
      "[[  90.2812871    36.64948258   97.04085794 -125.41752433  -28.19064734\n",
      "    76.28288223   97.37681826  -41.40195898  -56.92549775   17.58870599\n",
      "    27.37182208   73.34084621  211.19599252]\n",
      " [-263.95993746  -81.35192407 -134.50534288   85.42530184   40.65883345\n",
      "     6.9551471   154.53036184  100.2277951   -28.48638801 -310.80926881\n",
      "   114.21362832    7.64180817 -230.50397619]\n",
      " [ 135.67707929    4.84753801   85.28769102  100.2663828    -3.63503296\n",
      "   -22.82533178 -171.25578945  -35.53445471  -51.06959705  152.05944999\n",
      "  -176.79823346 -170.17637065   93.74735741]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianka/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_mh = linear_model.SGDClassifier(loss='modified_huber', penalty='none', random_state=0)\n",
    "model_mh.fit(X_train_std, y_train)\n",
    "y_pred = model_mh.predict(X_test_std)\n",
    "print('accuracy_score:{}\\nintercept_:\\n{}\\ncoef_:\\n{}'.format(accuracy_score(y_test, y_pred), model_mh.intercept_, model_mh.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:1.0\n",
      "intercept_:\n",
      "[-142.68148912 -125.44735328 -306.35815856]\n",
      "coef_:\n",
      "[[  89.79866954   36.16686502   96.55824038 -124.93490677  -27.70802979\n",
      "    75.80026467   96.8942007   -40.91934142  -56.44288019   17.10608844\n",
      "    26.88920452   72.85822865  210.71337496]\n",
      " [-263.4773199   -80.86930651 -134.02272532   84.94268428   40.17621589\n",
      "     6.47252954  154.04774428   99.74517754  -28.00377045 -310.32665125\n",
      "   113.73101076    7.15919062 -230.02135863]\n",
      " [ 154.81672112   16.21945299   36.93842907   36.94965086  -34.96832968\n",
      "    18.63499913 -146.8604452   -30.20730346  -51.9645466   169.96158763\n",
      "  -128.81638452 -135.2615544    97.21165791]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianka/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_mhl1 = linear_model.SGDClassifier(loss='modified_huber', penalty='l1', random_state=0)\n",
    "model_mhl1.fit(X_train_std, y_train)\n",
    "y_pred = model_mhl1.predict(X_test_std)\n",
    "print('accuracy_score:{}\\nintercept_:\\n{}\\ncoef_:\\n{}'.format(accuracy_score(y_test, y_pred), model_mhl1.intercept_, model_mhl1.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:0.9814814814814815\n",
      "intercept_:\n",
      "[-133.55229223 -108.84105874 -283.84845145]\n",
      "coef_:\n",
      "[[  64.24492318   27.99158523   68.97230425  -78.44154333   -9.43156164\n",
      "    53.00579554   68.95697172  -28.65065856  -31.27729694    9.10269409\n",
      "    18.97960851   52.32850534  142.62880064]\n",
      " [-173.82810269  -32.58102496 -120.3782499    28.25905445  -23.1259601\n",
      "     1.53120601   96.80522153   47.68519683  -44.39339951 -182.48718723\n",
      "    99.25512486   10.89635644 -163.40514175]\n",
      " [ 102.82149297   13.06803523   40.26527851   55.04226335    0.29059667\n",
      "   -56.00300297 -104.79731422  -25.38647002  -29.23814142  115.59206866\n",
      "  -134.68444213 -102.6787633    62.19846117]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianka/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_mhl2 = linear_model.SGDClassifier(loss='modified_huber', penalty='l2', random_state=0,)\n",
    "model_mhl2.fit(X_train_std, y_train)\n",
    "y_pred = model_mhl2.predict(X_test_std)\n",
    "print('accuracy_score:{}\\nintercept_:\\n{}\\ncoef_:\\n{}'.format(accuracy_score(y_test, y_pred), model_mhl2.intercept_, model_mhl2.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:1.0\n",
      "intercept_:\n",
      "[-115.76971554 -160.71704492 -285.30505012]\n",
      "coef_:\n",
      "[[ 123.91275499   27.25838834  122.67343871 -146.16408275  -33.48610385\n",
      "    17.85290524   26.40894794    0.          -29.98815536  -12.68503339\n",
      "    28.04845641   54.09734213  176.34210642]\n",
      " [-156.14636532  -12.2965563  -123.68993198   26.10839284    7.55016295\n",
      "    -6.98576902   81.06592956   53.72891307  -45.28760889 -242.95726581\n",
      "    78.15250959    7.51084648 -186.29067841]\n",
      " [ 112.36872187   17.4671006    43.78675923   40.4863028     3.30011746\n",
      "     3.5951165  -152.39547824   -4.13187172 -109.01228301  135.25016979\n",
      "  -102.56934514  -91.93306103   59.12231722]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianka/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.SGDClassifier(loss='modified_huber', penalty='elasticnet', random_state=0)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "print('accuracy_score:{}\\nintercept_:\\n{}\\ncoef_:\\n{}'.format(accuracy_score(y_test, y_pred), model.intercept_, model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss: 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', or a \n",
    "# regression loss: 'squared_loss', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
